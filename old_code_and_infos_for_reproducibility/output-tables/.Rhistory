install.packages("stargazer")
if(!require(mgcv)) {
install.packages("mgcv"); require(mgcv)}
if(!require(itsadug)) {
install.packages("itsadug"); require(itsadug)}
?start_event
?bam
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # for data carpentry
install.packages("tidyverse", dependencies=TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # for data carpentry
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # for data carpentry
install.packages("dplyr")
library(tidyverse) # for data carpentry
install.packages("tidyverse")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # for data carpentry
library(stringr) # for string processing
library(ggplot2) # for plots
library(ggpubr) # for
library(gmodels) # for
library(dplyr) # for
library(plyr)
### animations according to their expected assignment
bouba = c("ballaballa","dallidalli","dingdong","singsang","wirrwarr")
kiki = c("hopphopp","piffpaff","ratzfatz","ruckzuck","zickzack")
setwd("C:/Users/ola/OneDrive/PSIMS/2018-LNDW/log-bouba/")
file_list <- list.files()
for (file in file_list){
# if the merged dataset doesn't exist, create it
if (!exists("dataset")){
dataset <- read.table(file, header=TRUE, sep=",")
dataset$file=file
}
# if the merged dataset does exist, append to it
if (exists("dataset")){
temp_dataset <-read.table(file, header=TRUE, sep=",")
temp_dataset$file=file
dataset<-rbind(dataset, temp_dataset)
rm(temp_dataset)
}
}
## dump the stuff we don't need anymore
rm(file)
rm(file_list)
## change the name of the dataframe
df <- dataset
rm(dataset)
## First file is doubled, as it was loaded first as a start to a combined dataframe,
## so delete the first n rows (here n=11), depending on the lenght of your initial file
df=df[c(11:nrow(df)),]
## choose the columns you need
df <- df%>%
select(correct_keyboard_response,
correct_response,
response,
response_time,
target_word,
file
)
## make a column for subject
numextract <- function(string){
str_extract(string, "\\d+\\d*")
}
df$subject <- numextract(df$file)
## I also change subject-0 to subject-4 (the 4 was missing)
df$subject[df$subject=="0"] <- "4"
## now we can get rid of file column, cause we have it in subject
df <- df%>%
select(-file)
## change the column names to be shorter
colnames(df) <- c("correct","cor_key","response","rt","animation","subject")
## i will remove two more columns: cor_key and response,
## if they have identical values, the response was correct
## if not, it was incorrect; which is also depicted in cor_num
df <- df%>%
select(-cor_key,
-response)
## Another column of whether bouba or kiki was expected may be handy
df$target <- "bouba"
df$target[df$animation %in% kiki] <- "kiki"
df$target <- as.factor(df$target)
df %>% group_by(subject) %>%
summarise(ACC = mean(correct)) %>%
mutate(Percentage = str_c(round(correct, 2) * 100, '%'))
group_by(df, subject) %>%
summarise(mean=mean(correct), sd=sd(correct))
data.frame(df) %>%
group_by(subject) %>%
summarise(mean=mean(correct), sd=sd(correct))
aggregate(df$correct, list(df$subject), mean)
df %>%
group_by(subject) %>%
summarise_at(vars(correct), funs(mean(., na.rm=TRUE)))
df %>%
group_by(subject) %>%
summarise_at(vars(round(correct, 2)), funs(mean(., na.rm=TRUE)))
df %>% group_by(subject) %>%
summarise(ACC = mean(correct)) %>%
mutate(Percentage = str_c(round(correct, 2) * 100, '%'))
install.packages("C:/Users/ola/Desktop/stringi_1.2.4.zip", repos = NULL, type = "win.binary")
install.packages("yaml")
install.packages("C:/Users/ola/Desktop/stringi_1.2.4.zip", repos = NULL, type = "win.binary")
Sys.getenv("R_LIBS_USER")
ddply(df,~subject,summarise,mean=mean(correct),sd=sd(correct))
head(df)
str(df)
acc<-ddply(df,~subject,summarise,mean=mean(correct),sd=sd(correct))
print(acc)
acc.t<-ddply(df,~target,summarise,mean=mean(correct),sd=sd(correct))
print(acc.t)
acc.a<-ddply(df,~animation,summarise,mean=mean(correct),sd=sd(correct))
print(acc.a)
chisq.test(df$animation,df$correct)
chisq.test(df$target,df$correct)
ggplot(df, aes(animation, correct)) +
geom_bar(stat = "identity", fill = "darkblue") +
scale_x_discrete("words")+
scale_y_discrete("correct responses")+
theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
labs(title = "Bar Chart")
xmdl <- glmer(correct ~ 1 +
(1|subject) + (1|animation),
data = df, family = binomial)
library(lme4) # for mixed models
library(afex) # for likelihood ratio test, fixed effects
xmdl <- glmer(correct ~ 1 +
(1|subject) + (1|animation),
data = df, family = binomial)
summary(xmdl)
df %>% group_by(subject) %>%
summarise(ACC = mean(correct)) %>%
mutate(Percentage = round(correct, 2) * 100, '%')
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # for data carpentry
library(stringr) # for string processing
library(lme4) # for mixed models
library(afex) # for likelihood ratio test, fixed effects
library(RLRsim) # for likelihood ratio tests, random effects, not used yet
library(brms) # for bayesian models, not used yet
# Load the main data:
df <- read_delim('../data/experimentInputs_2018-10-24.txt',
delim = '\t', col_names = FALSE)
# Load column names and append them:
these_cols <- readLines('../data/header.txt')
colnames(df) <- these_cols
# Show first few columns:
df %>% print(n = 2, width = Inf)
table(df$Experiment)
df <- rename(df,
Subject = Session)
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # for data carpentry
library(stringr) # for string processing
library(lme4) # for mixed models
library(afex) # for likelihood ratio test, fixed effects
library(RLRsim) # for likelihood ratio tests, random effects, not used yet
library(brms) # for bayesian models, not used yet
# Load the main data:
df <- read_delim('../data/experimentInputs_2018-10-24.txt',
delim = '\t', col_names = FALSE)
# Load column names and append them:
these_cols <- readLines('../data/header.txt')
colnames(df) <- these_cols
# Show first few columns:
df %>% print(n = 2, width = Inf)
table(df$Experiment)
df <- rename(df,
Subject = Session)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # for data carpentry
library(stringr) # for string processing
library(ggplot2) # for plots
library(ggpubr) # for
library(gmodels) # for
library(dplyr) # for
library(plyr)
library(lme4) # for mixed models
library(afex) # for likelihood ratio test, fixed effects
### animations according to their expected assignment
bouba = c("ballaballa","dallidalli","dingdong","singsang","wirrwarr")
kiki = c("hopphopp","piffpaff","ratzfatz","ruckzuck","zickzack")
setwd("C:/Users/ola/OneDrive/PSIMS/2018-LNDW/log-bouba/")
file_list <- list.files()
for (file in file_list){
# if the merged dataset doesn't exist, create it
if (!exists("dataset")){
dataset <- read.table(file, header=TRUE, sep=",")
dataset$file=file
}
# if the merged dataset does exist, append to it
if (exists("dataset")){
temp_dataset <-read.table(file, header=TRUE, sep=",")
temp_dataset$file=file
dataset<-rbind(dataset, temp_dataset)
rm(temp_dataset)
}
}
## dump the stuff we don't need anymore
rm(file)
rm(file_list)
## change the name of the dataframe
df <- dataset
rm(dataset)
## First file is doubled, as it was loaded first as a start to a combined dataframe,
## so delete the first n rows (here n=11), depending on the lenght of your initial file
df=df[c(11:nrow(df)),]
## choose the columns you need
df <- df%>%
select(correct_keyboard_response,
correct_response,
response,
response_time,
target_word,
file
)
## make a column for subject
numextract <- function(string){
str_extract(string, "\\d+\\d*")
}
df$subject <- numextract(df$file)
## I also change subject-0 to subject-4 (the 4 was missing)
df$subject[df$subject=="0"] <- "4"
## now we can get rid of file column, cause we have it in subject
df <- df%>%
select(-file)
## change the column names to be shorter
colnames(df) <- c("correct","cor_key","response","rt","animation","subject")
## i will remove two more columns: cor_key and response,
## if they have identical values, the response was correct
## if not, it was incorrect; which is also depicted in cor_num
df <- df%>%
select(-cor_key,
-response)
## Another column of whether bouba or kiki was expected may be handy
df$target <- "bouba"
df$target[df$animation %in% kiki] <- "kiki"
df$target <- as.factor(df$target)
#df %>% group_by(subject) %>%
#  summarise(ACC = mean(correct)) %>%
#  mutate(Percentage = round(correct, 2) * 100, '%')
#(df, subject) %>%
# summarise(mean=mean(correct), sd=sd(correct))
#data.frame(df) %>%
#  group_by(subject) %>%
#  summarise(mean=mean(correct), sd=sd(correct))
acc.s<-ddply(df,~subject,summarise,mean=mean(correct),sd=sd(correct))
print(acc.s)
#aggregate(df$correct, list(df$subject), mean)
#df %>%
#  group_by(subject) %>%
#  summarise_at(vars(correct), funs(mean(., na.rm=TRUE)))
acc.t<-ddply(df,~target,summarise,mean=mean(correct),sd=sd(correct))
print(acc.t)
acc.a<-ddply(df,~animation,summarise,mean=mean(correct),sd=sd(correct))
print(acc.a)
chisq.test(df$animation,df$correct)
chisq.test(df$target,df$correct)
xmdl <- glmer(correct ~ 1 +
(1|subject) + (1|target),
data = df, family = binomial)
summary(xmdl)
md.tg <- glmer(correct ~ 1+target +
(1|subject) + (1|animation),
data = df, family = binomial)
summary(md.tg)
exp(fixef(xmdl))
plogis(fixef(xmdl))
summary(xmdl)$varcor
xmdl_concept <- mixed(correct ~ target +
(1|subject) + (1|animation),
data = df, family = binomial,
method = 'LRT')
xmdl_concept
ggplot(df, aes(animation, correct)) +
geom_bar(stat = "identity", fill = "darkblue") +
scale_x_discrete("words")+
scale_y_discrete("correct responses")+
theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
labs(title = "Bar Chart")
mean(df$correct)
sd(df$correct)
summary(xmdl)
md.tg <- glmer(correct ~ 1+target +
(1|subject),
data = df, family = binomial)
summary(md.tg)
md.tg <- glmer(correct ~ 1 +
(1|subject),
data = df, family = binomial)
summary(md.tg)
md.tg <- glmer(correct ~ 1 +
(1|subject) + (1|animation),
data = df, family = binomial)
summary(md.tg)
x=1:5
library(lattice)
library(car)
library(ggplot2)
library(ggpubr) # for the correlation plot from
#http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r
library(lme4)
## RePsychLing has to be downloaded from an extern repository,
## as it is not included in R packages repository.
## https://github.com/dmbates/RePsychLing
## Used to validate if the model if not overspecified.
library(RePsychLing)
## Use the following package to export output tables to LaTeX
#library(stargazer)
library(dplyr)
## Use the following package to export output tables to LaTeX
#install.packages("stargazer")
library(stargazer)
#### Load data ####
rm(list=ls())
setwd("C:/Users/ola/OneDrive/PSIMS/2018-f0-can/analysis/")
df=read.csv('analysis_new.csv', sep=',')
subs=read.csv('personal-info.csv',sep='\t')
summary(subs)
subs$dataset<-as.factor(subs$dataset)
subs$ID=factor(substr(subs$ID,3,4))
names(subs)[1] <- "subject"
### Vowels only, so subset and convert the vowel factor
### into numeric variable for the LM (the second point is optional)
nd <- subset(df, text=='I'|text=='a')
### Faktoren neu berechnen um leere Faktoren zu löschen
nd$text <- factor(nd$text)
summary(nd)
str(nd)
### Subset for better management
nd <- nd[,c("tmin","text","maxF0","meanF0","minF0","subject",
"part","size","v.pos","h.pos","medianF1","medianF2",
"F0range","slopeF0","LipClos","LipOpen","Point_y_rw","y_mg")]
#### Make sure this works! ####
### Merge the dataset with subject information
nd<-merge(nd,subs,by="subject")
nd$subject<-as.factor(nd$subject)
nd$part<-as.factor(nd$part)
### Subject 7 behaved in the opposite manner, so omit her for now
nd <- subset(nd, subject!='7')
summary(nd)
rm(list=ls());cat("\014")
##### START HERE #####
## initially planned clean-up
## was done in python
## see clean-up.ipynb
##### Clean-up of redundant data #####
## see: problem 1 and 2 in readme
## A function recoding the variables in numeric variables
recode=function(dataset) {
dataset[["duration"]]=as.numeric(as.character(dataset[["duration"]]))
dataset[["maxF0"]]=as.numeric(as.character(dataset[["maxF0"]]))
dataset[["meanF0"]]=as.numeric(as.character(dataset[["meanF0"]]))
dataset[["meanF0st"]]=as.numeric(as.character(dataset[["meanF0st"]]))
dataset[["medianF0"]]=as.numeric(as.character(dataset[["medianF0"]]))
dataset[["minF0"]]=as.numeric(as.character(dataset[["minF0"]]))
dataset[["sdF0"]]=as.numeric(as.character(dataset[["sdF0"]]))
dataset[["midpointF0"]]=as.numeric(as.character(dataset[["midpointF0"]]))
dataset[["slopeF0"]]=as.numeric(as.character(dataset[["slopeF0"]]))
dataset[["maxInt"]]=as.numeric(as.character(dataset[["maxInt"]]))
dataset[["meanInt"]]=as.numeric(as.character(dataset[["meanInt"]]))
dataset[["medianInt"]]=as.numeric(as.character(dataset[["medianInt"]]))
dataset[["minInt"]]=as.numeric(as.character(dataset[["minInt"]]))
dataset[["sdInt"]]=as.numeric(as.character(dataset[["sdInt"]]))
dataset[["midpointInt"]]=as.numeric(as.character(dataset[["midpointInt"]]))
# dataset[["maxF1"]]=as.numeric(as.character(dataset[["maxF1"]]))
# dataset[["meanF1"]]=as.numeric(as.character(dataset[["meanF1"]]))
dataset[["medianF1"]]=as.numeric(as.character(dataset[["medianF1"]]))
dataset[["medianF1Bark"]]=as.numeric(as.character(dataset[["medianF1Bark"]]))
# dataset[["minF1"]]=as.numeric(as.character(dataset[["minF1"]]))
# dataset[["sdF1"]]=as.numeric(as.character(dataset[["sdF1"]]))
# dataset[["midpointF1"]]=as.numeric(as.character(dataset[["midpointF1"]]))
# dataset[["maxF2"]]=as.numeric(as.character(dataset[["maxF2"]]))
# dataset[["meanF2"]]=as.numeric(as.character(dataset[["meanF2"]]))
dataset[["medianF2"]]=as.numeric(as.character(dataset[["medianF2"]]))
dataset[["medianF2Bark"]]=as.numeric(as.character(dataset[["medianF2Bark"]]))
# dataset[["minF2"]]=as.numeric(as.character(dataset[["minF2"]]))
# dataset[["sdF2"]]=as.numeric(as.character(dataset[["sdF2"]]))
# dataset[["midpointF2"]]=as.numeric(as.character(dataset[["midpointF2"]]))
dataset[["SPLH.SPL"]]=as.numeric(as.character(dataset[["SPLH.SPL"]]))
return(dataset)
}
## Load data
setwd("C:/Users/ola/OneDrive/PSIMS/2018-f0-can/data/output-tables/new/")
file_list <- list.files()
for (file in file_list){
# if the merged dataset doesn't exist, create it
if (!exists("dataset")){
dataset <- read.table(file, header=TRUE, sep=",")
dataset=recode(dataset)
dataset$file=file
}
# if the merged dataset does exist, append to it
if (exists("dataset")){
temp_dataset <-read.table(file, header=TRUE, sep=",")
temp_dataset=recode(temp_dataset)
temp_dataset$file=file
dataset<-rbind(dataset, temp_dataset)
rm(temp_dataset)
}
}
## dump the stuff we don't need anymore
rm(file)
rm(file_list)
## change the name of the dataframe
df <- dataset
rm(dataset)
## some of the annotations had space or tabs in them, they need to be replaced before subsetting
data.frame(table(df$text))
df$text <- gsub("p\037", "p", df$text)
df$text <- gsub("a\037", "a", df$text)
df$text <- gsub("I\037", "I", df$text)
df$text <- gsub("f\037", "f", df$text)
df$text <- gsub("a\037\037", "a", df$text)
df$text <- gsub("I\037\037", "I", df$text)
df$text <- gsub("p\t", "p", df$text)
df$text <- gsub("f\t", "f", df$text)
df$text <- gsub("a\037", "a", df$text)
df$text <- gsub("I\037", "I", df$text)
## some of the annotations had space or tabs in them, they need to be replaced before subsetting
data.frame(table(df$text))
ndf <- subset(df, text=='p'|text=='I'|text=='a'|text=='f')
data.frame(table(ndf$text))
data.frame(table(ndf$file))
ndf=ndf[c(151:nrow(ndf)),]
data.frame(table(ndf$text))
data.frame(table(ndf$file))
sum(is.na(ndf))
ndf$subject = substr(ndf$file, 3, nchar(ndf$file)-10)
unique(ndf$subject)
ndf$subject <- as.factor(ndf$subject)
#### STOP HERE ####
### Save the table
write.csv(ndf, file = "all.csv",row.names=FALSE)
rm(list=ls())
setwd("C:/Users/ola/OneDrive/PSIMS/2018-f0-can/data/output-tables/")
df=read.csv('all.csv', sep='\t', dec = ",")
library(stringr)
summary(df)
## add column for part (can be used as random factor)
df$part=str_sub(df$file,-5,-5)
## add column for can size
df$size=substr(df$condition,1,1)
unique(df$size)
## from 1=high to 5=low
df$v.pos=substr(df$condition,3,3)
## from 1=left to 5=right
df$h.pos=substr(df$condition,4,4)
df$size <- as.factor(df$size)
df$v.pos <- as.factor(df$v.pos)
df$h.pos <- as.factor(df$h.pos)
## subject is integer, so change it to factor
df$subject <- as.factor(df$subject)
#### Optimize the parameters ####
## Normalization ----
## Semitones
spkrs=unique(df$subject)
df$st_maxF0=0
df$st_minF0=0
F0_meds=tapply(df[df$text=='a'|df$text=='I' & is.na(df$maxF0)==FALSE & is.na(df$medianF0)==FALSE,]$medianF0,
df[df$text=='a'|df$text=='I' & is.na(df$maxF0)==FALSE & is.na(df$medianF0)==FALSE,]$subject,
median)
semi=log(2^(1/12))
for (sp in 1:length(spkrs)) {
df[df$subject==spkrs[sp],]$st_maxF0=(log(df[df$subject==spkrs[sp],]$maxF0)-log(F0_meds[sp]))/semi
df[df$subject==spkrs[sp],]$st_minF0=(log(df[df$subject==spkrs[sp],]$minF0)-log(F0_meds[sp]))/semi
}
df$st_F0range=df$st_maxF0-df$st_minF0
## Gender
df$F1centroid=600
df$F2centroid=1800
#data[data$sex=='m',]$F1centroid=500
#data[data$sex=='m',]$F2centroid=1500
df$F1centroidBark=13*atan(0.00076*df$F1centroid)+3.5*atan(df$F1centroid/7500)^2
df$F2centroidBark=13*atan(0.00076*df$F2centroid)+3.5*atan(df$F2centroid/7500)^2
df$vt_expansion=sqrt(abs(df$medianF1-df$F1centroid)^2+abs(df$medianF2-df$F2centroid)^2)
df$vt_expansionBark=sqrt(abs(df$medianF1Bark-df$F1centroidBark)^2+abs(df$medianF2Bark-df$F2centroidBark)^2)
## Spectral parameters
df$SPLH.SPL.dB=10*log10(df$SPLH.SPL)
df$F0range=df$maxF0-df$minF0
summary(df$SPLH.SPL.dB)
summary(df$SPLH.SPL)
write.csv(df, file = "analysis.csv",row.names=FALSE)
#### THE END ####
summary(df)
str(df)
## MoCap sampling rate
df$Fs <- 200
df[df$subject=="8"|df$subject=="9"|df$subject=="10"|df$subject=="11"|df$subject=="12"|df$subject=="13"|df$subject=="14"]$Fs=120
df[df$subject=="8"|df$subject=="9"|df$subject=="10"|df$subject=="11"|df$subject=="12"|df$subject=="13"|df$subject=="14",]$Fs=120
df$Fs
with(df,table(subject,Fs))
## Handedness
df$hand<-"r"
df[df$subject=="12",]$hand="l"
with(df,table(subject,hand))
write.csv(df, file = "analysis.csv",row.names=FALSE)
## Extract vowels only
dfv <- subset(df, text=='I'|text=='a')
write.csv(dfv, file = "analysis_vowel.csv",row.names=FALSE)
## Extract consonants only
dfc <- subset(df, text=='p'|text=='f')
write.csv(dfc, file = "analysis_vowel.csv",row.names=FALSE)
## some of the annotations had space or tabs in them, they need to be replaced before subsetting
data.frame(table(df$text))
write.csv(dfv, file = "analysis_vow.csv",row.names=FALSE)
write.csv(dfc, file = "analysis_cons.csv",row.names=FALSE)
## Extract all segments
write.csv(df, file = "analysis_all.csv",row.names=FALSE)
