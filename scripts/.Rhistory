# Chunk 8
can
# Chunk 9
can <- subset(can, speaker != '16')
# Chunk 10
# Columns to process
columns_to_process <- c("dB_mean", "f0_range", "f0_mean", "F1", "F2", "F3")
# Group by Language and Participant and then loop through each column
can_grouped <- can %>%
group_by(speaker, vowel) %>%
mutate(across(all_of(columns_to_process), .fns = ~{
cat("Processing column:", deparse(substitute(.)), "\n")
# Calculate IQR
Q1 <- quantile(.x, 0.25, na.rm = TRUE)
Q3 <- quantile(.x, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
# Define lower and upper bounds for outliers
lower_bound <- Q1 - 2 * IQR
upper_bound <- Q3 + 2 * IQR
# Identify outliers
outliers <- .x < lower_bound | .x > upper_bound
# Replace outliers with NA
.x[outliers] <- NA
.x
})) %>%
ungroup()
# Chunk 11
# Combine the results into a data frame
na_summary <- data.frame(
NA_Count_Pre_Removal = colSums(is.na(can[, columns_to_process])),
NA_Count_Post_Removal = colSums(is.na(can_grouped[, columns_to_process])),
NA_Perc_Pre_Removal = (colSums(is.na(can[, columns_to_process])) / nrow(can)) * 100,
NA_Perc_Post_Removal = (colSums(is.na(can_grouped[, columns_to_process])) / nrow(can_grouped)) * 100
)
na_summary
# Chunk 12
# Calculate means for each variable in the pre-removal dataset
means_can <- can %>%
group_by(speaker, vowel) %>%
summarise(across(all_of(columns_to_process), mean, na.rm = TRUE)) %>%
ungroup()
# Calculate means for each variable in the post-removal dataset
means_can_grouped <- can_grouped %>%
group_by(speaker, vowel) %>%
summarise(across(all_of(columns_to_process), mean, na.rm = TRUE)) %>%
ungroup()
# Combine the means into a single summary table
outlier_summary <- bind_rows(
tibble(Variable = "Pre-Removal", means_can),
tibble(Variable = "Post-Removal", means_can_grouped)
)
# Chunk 13
can <- can_grouped
rm(can_grouped, means_can, means_can_grouped)
rm(columns_to_process)
# Chunk 14
# Create tibble with means and sd per speaker and vowel:
can %>%
na.omit() %>%
group_by(speaker, vowel) %>%
summarize_at(vars(dB_mean, f0_mean, f0_range, F1, F2, F3), list(mean = mean, sd = sd)) %>%
ungroup() %>%
{. ->> speakers_phon}
# Join tibble with means per speaker and vowel
can <- can %>%
left_join(speakers_phon, by = c("speaker", "vowel")) %>%
# Calculate z-scores
mutate(
db_mean_z = (dB_mean - dB_mean_mean) / dB_mean_sd,
f0_mean_z = (f0_mean - f0_mean_mean) / f0_mean_sd,
f0_range_z = (f0_range - f0_range_mean) / f0_range_sd,
F1_z = (F1 - F1_mean) / F1_sd,
F2_z = (F2 - F2_mean) / F2_sd,
F3_z = (F3 - F3_mean) / F3_sd
) %>%
# Deselect unneeded variables
select(-c(
dB_mean_sd, dB_mean_mean,
f0_mean_sd, f0_mean_mean,
f0_range_sd, f0_range_mean,
F1_sd, F1_mean,
F2_sd, F2_mean,
F3_sd, F3_mean
))
# Chunk 15
# Create tibble with means and sd per speaker and vowel for head_pos, lip_dist, and angle:
can %>%
na.omit() %>%
group_by(speaker, vowel) %>%
summarize_at(vars(head_pos, lip_dist, angle), list(mean = mean, sd = sd)) %>%
ungroup() %>%
{. ->> speakers_kin}
# Join tibble with means per speaker and vowel
can <- can %>%
left_join(speakers_kin, by = c("speaker", "vowel")) %>%
# Calculate z-scores
mutate(
head_pos_z = (head_pos - head_pos_mean) / head_pos_sd,
lip_dist_z = (lip_dist - lip_dist_mean) / lip_dist_sd,
angle_z = (angle - angle_mean) / angle_sd
) %>%
# deselect unneeded variables
select(-c(
head_pos_sd, head_pos_mean,
lip_dist_sd, lip_dist_mean,
angle_sd, angle_mean
))
# Chunk 16
can %>%
group_by(speaker) %>%
summarize_at(vars(angle), list(mean = mean, sd = sd))%>%
{. ->> speakers_angle} %>%
ungroup() %>%
print(n = Inf)
# Chunk 17
mean(speakers_angle$sd)
# Chunk 18
filter(speakers_angle, sd > 2.5)
# Chunk 19
filter(speakers_angle, sd < 2.5)
# Chunk 20
can <- left_join(can,
can %>%
group_by(speaker) %>%
summarize_at(vars(angle), list(mean_angle = mean, sd_angle = sd)) %>%
mutate(mover = ifelse(sd_angle < 2.5, 'no', 'yes')) %>%
ungroup())
# Chunk 21
can <- can %>%
mutate(angle_difference = angle - mean_angle) %>%
select(-c(sd_angle, mean_angle)) # deselect unneded variables
# Chunk 22
can_noNA <- can %>%
mutate(across(c(F1, F2, F3), ~ifelse(is.na(.), 9999, .)))
# Chunk 23
can_noNA <- mutate(can_noNA,
F1_bark = normBark(F1),
F2_bark = normBark(F2),
F3_bark = normBark(F3))
# Chunk 24
can_noNA <- mutate(can_noNA,
across(c(F1, F2, F3), ~ifelse(. == 9999, NA, .)),
across(c(F1_bark, F2_bark, F3_bark),
~ifelse(is.na(get(substring(cur_column(), 1, 2))), NA, .)))
can <- can_noNA
rm(can_noNA)
# Chunk 25
# Pivot the data to create separate columns for each variable and category
can_formants_aside <- can %>%
pivot_wider(
id_cols = c(speaker, vowel, v_pos, h_pos),
names_from = can_size,
values_from = c(F1, F2, F3, lip_dist, F1_z, F2_z, F3_z, F1_bark, F2_bark, F3_bark),
names_sep = "_"
) %>%
# Rename columns for clarity
rename(
F1_large = F1_large,
F1_small = F1_small,
F2_large = F2_large,
F2_small = F2_small,
F3_large = F3_large,
F3_small = F3_small,
lip_dist_large = lip_dist_large,
lip_dist_small = lip_dist_small,
F1_z_large = F1_z_large,
F1_z_small = F1_z_small,
F2_z_large = F2_z_large,
F2_z_small = F2_z_small,
F3_z_large = F3_z_large,
F3_z_small = F3_z_small,
F1_bark_large = F1_bark_large,
F1_bark_small = F1_bark_small,
F2_bark_large = F2_bark_large,
F2_bark_small = F2_bark_small,
F3_bark_large = F3_bark_large,
F3_bark_small = F3_bark_small
)
# View the resulting dataframe
print(can_formants_aside)
# Chunk 26
can_formants_aside <- mutate(can_formants_aside,
dist_size_hz = sqrt((F1_large - F1_small)^2 + (F2_large - F2_small)^2),
dist_size_z = sqrt((F1_z_large - F1_z_small)^2 + (F2_z_large - F2_z_small)^2),
dist_size_bark = sqrt((F1_bark_large - F1_bark_small)^2 + (F2_bark_large - F2_bark_small)^2)
)
# Chunk 27
length(unique(can$speaker))
# Chunk 28
can %>%
group_by(speaker) %>%
summarize(num_unique_realizations = n_distinct(paste(can_size, v_pos, h_pos, vowel))) %>%
ungroup() %>%
print(n = 30)
# Chunk 29
can %>%
group_by(speaker) %>%
summarize(
across(c(dB_mean, f0_mean, f0_range, F1, F2, F3, head_pos, lip_dist, angle),
~sum(is.na(.))),
.groups = 'drop'
) %>%
print(n = 30)
# Chunk 30
print(speakers_phon, n = 60)
# Chunk 31
print(speakers_kin, n = 60)
# Chunk 32
cor.test(can$angle, can$head_pos)
# what will happen if we use z-normalized values
cor.test(can$angle_z, can$head_pos_z)
# the correlation skyrocketed! but it was there anyway
# Chunk 33
ggplot(can,
aes(x = head_pos,
y = angle)) +
geom_point(alpha = 0.2) +
geom_smooth(size = 1,
method = lm,
se = F)
# Chunk 34
ggplot(can,
aes(x = head_pos_z,
y = angle_z)) +
geom_point(alpha = 0.2) +
geom_smooth(size = 1,
method = lm,
se = F)
# Chunk 35
# Plot for head_pos vs f0_mean_z
f0_head_pos <- ggplot(can,
aes(x = f0_mean_z,
y = head_pos_z)) +
geom_point(alpha = 0.2) +
geom_smooth(size = 1,
method = lm,
se = FALSE) +
labs(title = "Head position vs f0 mean")
# Plot for angle vs f0_mean_z
f0_angle <- ggplot(can,
aes(x = f0_mean_z,
y = angle_z)) +
geom_point(alpha = 0.2) +
geom_smooth(size = 1,
method = lm,
se = FALSE) +
labs(title = "Angle vs f0 mean")
# Arrange plots side by side
grid.arrange(f0_head_pos, f0_angle, ncol = 2)
# Chunk 36
cor.test(can$head_pos, can$v_pos)
# what will happen if we use z-normalized values
cor.test(can$head_pos_z, can$v_pos)
# Chunk 37
ggplot(can,
aes(x = v_pos,
y = head_pos)) +
geom_point(alpha = 0.2) +
geom_smooth(size = 1,
method = lm,
se = F)
# Chunk 38
can <- can %>%
mutate(
vowel_s = if_else(vowel == "I", -0.5, 0.5),
can_size_s = if_else(can_size == "small", -0.5, 0.5),
mover_s = if_else(mover == "no", -0.5, 0.5)
)
# Chunk 39
get_prior(formula = f0_mean ~ 1,
data = can,
family = lognormal())
# Chunk 40
# If we assume that the mean for f0 is 200
log(200)
# If we set the prior to normal(0,1), what would be the boundaries
exp(log(200)-0.5); exp(log(200)+0.5)
# If we set the prior to normal(0,2), what would be the boundaries
exp(log(200)-1); exp(log(200)+1)
# If we set the prior to normal(0,3), what would be the boundaries
exp(log(200)-1.5); exp(log(200)+1.5)
# I think that for (0,3) we can safely assume to respect all datapoints. This will be the prior we choose for the intercept.
# Chunk 41
priors_intOnlyH1 <- c(
prior('normal(0, 3)', class = 'Intercept')
)
# Chunk 42
get_prior(formula = f0_mean ~ 1 + vowel_s + can_size_s + head_pos + h_pos + v_pos + height + # angle and v_pos will be separated
(1 | speaker),
data = can,
family = lognormal())
# Chunk 43
priors_noRandomH1 <- c(
prior('normal(0, 3)', class = 'Intercept'),
prior('normal(0, 1)', class = b)
)
# Chunk 44
get_prior(formula = f0_mean ~ 1 + vowel_s + can_size_s + head_pos + h_pos + v_pos + height + # head_pos and v_pos will be separated
(1 + vowel_s + can_size_s + head_pos + h_pos + v_pos || speaker),
data = can,
family = lognormal())
# Chunk 45
priors_maxH1 <- c(
prior('normal(0, 3)', class = 'Intercept'),
prior('normal(0, 1)', class = b)
)
# Chunk 46
mdl_intOnlyH1 <- brm(f0_mean ~ 1,
data = can,
prior = priors_intOnlyH1,
family = lognormal(),
#backend = "cmdstanr", # depends on you
cores = 4,
chains = 4,
iter = 8000,
warmup = 4000,
seed = 998,
control = list(max_treedepth = 13,
adapt_delta = 0.99),
file = paste0(models, "mdl_intOnlyH1.rds"))
# if we need to compress the model more
#saveRDS(mdl_intOnlyH1, file = paste0(models, "mdl_intOnlyH1.rds"), compress = "xz")
mdl_intOnlyH1 <- readRDS(paste0(models, "mdl_intOnlyH1.rds"))
# Chunk 47
summary(mdl_intOnlyH1)
# Chunk 48
plot(mdl_intOnlyH1)
# Chunk 49
pp_check(mdl_intOnlyH1, ndraws = 100)
# Chunk 50
# run loo mdl
if (file.exists(paste0(models, "mdl_intOnlyH1_loo.rds"))) {
mdl_intOnlyH1_loo <- readRDS(paste0(models, "mdl_intOnlyH1_loo.rds"))
} else {
mdl_intOnlyH1_loo <- loo(mdl_intOnlyH1)
saveRDS(mdl_intOnlyH1_loo, paste0(models, "mdl_intOnlyH1_loo.rds"))
}
# Chunk 51
mdl_noRandomH1_headPos <- brm(f0_mean ~ 1 + vowel_s + can_size_s +
head_pos + h_pos + height +
(1 | speaker),
data = can,
prior = priors_noRandomH1,
family = lognormal(),
#backend = "cmdstanr",
cores = 4,
chains = 4,
iter = 8000,
warmup = 4000,
seed = 997,
save_pars = save_pars(all = TRUE),
control = list(max_treedepth = 13,
adapt_delta = 0.99),
file = paste0(models, "mdl_noRandomH1_headPos.rds"))
# if we need to compress the model more
#saveRDS(mdl_noRandomH1_headPos, file = paste0(models, "mdl_noRandomH1_headPos.rds"), compress = "xz")
mdl_noRandomH1_headPos <- readRDS(paste0(models, "mdl_noRandomH1_headPos.rds"))
# Chunk 52
summary(mdl_noRandomH1_headPos)
# Chunk 53
plot(mdl_noRandomH1_headPos)
# Chunk 54
conditional_effects(mdl_noRandomH1_headPos, sample_prior = "only")
# Chunk 56
# run loo mdl
if (file.exists(paste0(models, "mdl_noRandomH1_headPos_loo.rds"))) {
mdl_noRandomH1_headPos_loo <- readRDS(paste0(models, "mdl_noRandomH1_headPos_loo.rds"))
} else {
mdl_noRandomH1_headPos_loo <- loo(mdl_noRandomH1_headPos)
saveRDS(mdl_noRandomH1_headPos_loo, paste0(models, "mdl_noRandomH1_headPos_loo.rds"))
}
# Chunk 57
mdl_noRandomH1_vPos <- brm(f0_mean ~ 1 + vowel_s + can_size_s +
h_pos + v_pos + height +
(1 | speaker),
data = can,
prior = priors_noRandomH1,
family = lognormal(),
#backend = "cmdstanr",
cores = 4,
chains = 4,
iter = 8000,
warmup = 4000,
seed = 998,
control = list(max_treedepth = 13,
adapt_delta = 0.99),
file = paste0(models, "mdl_noRandomH1_vPos.rds"))
# if we need to compress the model more
#saveRDS(mdl_noRandomH1_vPos, file = paste0(models, "mdl_noRandomH1_vPos.rds"), compress = "xz")
mdl_noRandomH1_vPos <- readRDS(paste0(models, "mdl_noRandomH1_vPos.rds"))
# Chunk 58
summary(mdl_noRandomH1_vPos)
# Chunk 59
plot(mdl_noRandomH1_vPos)
# Chunk 60
conditional_effects(mdl_noRandomH1_vPos, sample_prior = "only")
# Chunk 62
# run loo mdl
if (file.exists(paste0(models, "mdl_noRandomH1_vPos_loo.rds"))) {
mdl_noRandomH1_vPos_loo <- readRDS(paste0(models, "mdl_noRandomH1_vPos_loo.rds"))
} else {
mdl_noRandomH1_vPos_loo <- loo(mdl_noRandomH1_vPos)
saveRDS(mdl_noRandomH1_vPos_loo, paste0(models, "mdl_noRandomH1_vPos_loo.rds"))
}
# Chunk 63
mdl_maxH1_headPos <- brm(f0_mean ~ 1 + vowel_s + can_size_s +
head_pos + h_pos + height +
(1 + vowel_s + can_size_s +
head_pos + h_pos || speaker),
data = can,
prior = priors_maxH1,
family = lognormal(),
#backend = "cmdstanr",
cores = 4,
chains = 4,
iter = 8000,
warmup = 4000,
seed = 998,
save_pars = save_pars(all = TRUE),
control = list(max_treedepth = 13,
adapt_delta = 0.99),
file = paste0(models, "mdl_maxH1_headPos.rds"))
# if we need to compress the model more
#saveRDS(mdl_maxH1_headPos, file = paste0(models, "mdl_maxH1_headPos.rds"), compress = "xz")
mdl_maxH1_headPos <- readRDS(paste0(models, "mdl_maxH1_headPos.rds"))
beepr::beep()
# Chunk 64
summary(mdl_maxH1_headPos)
# Chunk 65
plot(mdl_maxH1_headPos)
conditional_effects(mdl_maxH1_headPos, sample_prior = "only")
mdl_intOnlyH1_loo
mdl_noRandomH1_headPos_loo
mdl_noRandomH1_headPos_reloo <- reloo(mdl_noRandomH1_headPos_loo, mdl_noRandomH1_headPos, chains = 1)
# run loo mdl
if (file.exists(paste0(models, "mdl_noRandomH1_vPos_loo.rds"))) {
mdl_noRandomH1_vPos_loo <- readRDS(paste0(models, "mdl_noRandomH1_vPos_loo.rds"))
} else {
mdl_noRandomH1_vPos_loo <- loo(mdl_noRandomH1_vPos)
saveRDS(mdl_noRandomH1_vPos_loo, paste0(models, "mdl_noRandomH1_vPos_loo.rds"))
}
mdl_noRandomH1_vPos_loo
mdl_maxH1_headPos_loo
# run loo mdl
if (file.exists(paste0(models, "mdl_maxH1_headPos_loo.rds"))) {
mdl_maxH1_headPos_loo <- readRDS(paste0(models, "mdl_maxH1_headPos_loo.rds"))
} else {
mdl_maxH1_headPos_loo <- loo(mdl_maxH1_headPos, moment_match = TRUE)
saveRDS(mdl_maxH1_headPos_loo, paste0(models, "mdl_maxH1_headPos_loo.rds"))
}
mdl_maxH1_headPos_loo
mdl_maxH1_headPos_reloo <- reloo(mdl_maxH1_headPos_loo, mdl_maxH1_headPos, chains = 1)
(mdl_maxH1_headPos_reloo <- reloo(mdl_maxH1_headPos_loo, mdl_maxH1_headPos, chains = 4))
(mdl_maxH1_headPos_reloo <- reloo(mdl_maxH1_headPos_loo, mdl_maxH1_headPos, chains = 1))
mdl_maxH1_vPos <- brm(f0_mean ~ 1 + vowel_s + can_size_s +
h_pos + v_pos + height +
(1 + vowel_s + can_size_s +
h_pos + v_pos || speaker),
data = can,
prior = priors_maxH1,
family = lognormal(),
#backend = "cmdstanr",
cores = 4,
chains = 4,
iter = 8000,
warmup = 4000,
seed = 998,
save_pars = save_pars(all = TRUE),
control = list(max_treedepth = 13,
adapt_delta = 0.99),
file = paste0(models, "mdl_maxH1_vPos.rds"))
# if we need to compress the model more
#saveRDS(mdl_maxH1_vPos, file = paste0(models, "mdl_maxH1_vPos.rds"), compress = "xz")
mdl_maxH1_vPos <- readRDS(paste0(models, "mdl_maxH1_vPos.rds"))
beepr::beep()
summary(mdl_maxH1_vPos)
mdl_maxH1_vPos_loo
# run loo mdl
if (file.exists(paste0(models, "mdl_maxH1_vPos_loo.rds"))) {
mdl_maxH1_vPos_loo <- readRDS(paste0(models, "mdl_maxH1_vPos_loo.rds"))
} else {
mdl_maxH1_vPos_loo <- loo(mdl_maxH1_vPos, moment_match = TRUE)
saveRDS(mdl_maxH1_vPos_loo, paste0(models, "mdl_maxH1_vPos_loo.rds"))
}
mdl_maxH1_vPos_loo
(mdl_maxH1_vPos_reloo <- reloo(mdl_maxH1_vPoss_loo, mdl_maxH1_vPos, chains = 1))
(mdl_maxH1_vPos_reloo <- reloo(mdl_maxH1_vPoss_loo, mdl_maxH1_vPos, chains = 1))
(mdl_maxH1_vPos_reloo <- reloo(mdl_maxH1_vPos_loo, mdl_maxH1_vPos, chains = 1))
mdl_maxH1_vPos_reloo <- reloo(mdl_maxH1_vPos_loo, mdl_maxH1_vPos, chains = 1)
View(mdl_maxH1_vPos_reloo)
mdl_maxH1_headPos_loo
(mdl_maxH1_headPos_reloo <- reloo(mdl_maxH1_headPos_loo, mdl_maxH1_headPos, chains = 1))
summary(mdl_maxH1_vPos)
summary(mdl_maxH1_headPos)
summary(mdl_maxH1_vPos)
?hypothesis()
hypothesis(mdl_maxH1_vPos, "v_pos < 0")
hypothesis(mdl_maxH1_headPos, "head_pos < 0")
hypothesis(mdl_maxH1_headPos, "head_pos > 0")
mdl_maxH1_vPos_reloo <- reloo(mdl_maxH1_vPos_loo, mdl_maxH1_vPos, chains = 4)
mdl_maxH1_headPos_reloo <- reloo(mdl_maxH1_headPos_loo, mdl_maxH1_headPos, chains = 1)
saveRDS(mdl_maxH1_headPos_reloo, paste0(models, "mdl_maxH1_headPos_reloo.rds")
)
mdl_maxH1_headPos_reloo
saveRDS(mdl_noRandomH1_headPos_reloo, paste0(models, "mdl_noRandomH1_headPos_reloo.rds"))
saveRDS(mdl_maxH1_vPos_reloo, paste0(models, "mdl_maxH1_vPos_reloo.rds"))
compH1_hPos <- loo_compare(mdl_intOnlyH1_loo,mdl_noRandomH1_headPos_reloo,mdl_maxH1_headPos_reloo)
compH1_hPos
compH1_headPos <- loo_compare(mdl_intOnlyH1_loo,mdl_noRandomH1_headPos_reloo,mdl_maxH1_headPos_reloo)
compH1_headPos
compH1_vPos <- loo_compare(mdl_intOnlyH1_loo,mdl_noRandomH1_vPos_loo,mdl_maxH1_vPos_reloo)
compH1_vPos
